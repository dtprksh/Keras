{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 100\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 34s 0us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(shear_range=0.2,horizontal_flip=True,rotation_range=30,cval=0.2,width_shift_range=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 267s 171ms/step - loss: 1.4640 - acc: 0.4822 - val_loss: 1.4416 - val_acc: 0.4884\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 270s 173ms/step - loss: 1.1824 - acc: 0.5887 - val_loss: 1.3458 - val_acc: 0.5528\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 256s 164ms/step - loss: 1.0505 - acc: 0.6332 - val_loss: 1.1802 - val_acc: 0.6055\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 267s 171ms/step - loss: 0.9651 - acc: 0.6662 - val_loss: 0.8857 - val_acc: 0.6954\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.9054 - acc: 0.6876 - val_loss: 0.9587 - val_acc: 0.6679\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.8639 - acc: 0.7005 - val_loss: 0.8589 - val_acc: 0.7032\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.8234 - acc: 0.7138 - val_loss: 0.7849 - val_acc: 0.7357\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 251s 161ms/step - loss: 0.7929 - acc: 0.7276 - val_loss: 0.8167 - val_acc: 0.7233\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 252s 161ms/step - loss: 0.7594 - acc: 0.7379 - val_loss: 0.7482 - val_acc: 0.7489\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.7469 - acc: 0.7422 - val_loss: 0.7540 - val_acc: 0.7424\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 250s 160ms/step - loss: 0.7207 - acc: 0.7509 - val_loss: 0.7322 - val_acc: 0.7546\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 259s 166ms/step - loss: 0.7047 - acc: 0.7578 - val_loss: 0.7457 - val_acc: 0.7436\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 266s 170ms/step - loss: 0.6920 - acc: 0.7621 - val_loss: 0.6349 - val_acc: 0.7849\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 266s 170ms/step - loss: 0.6693 - acc: 0.7704 - val_loss: 0.8739 - val_acc: 0.7072\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 259s 166ms/step - loss: 0.6649 - acc: 0.7722 - val_loss: 0.6977 - val_acc: 0.7589\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.6476 - acc: 0.7770 - val_loss: 0.6304 - val_acc: 0.7827\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 270s 172ms/step - loss: 0.6365 - acc: 0.7801 - val_loss: 0.6819 - val_acc: 0.7665\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 262s 168ms/step - loss: 0.6253 - acc: 0.7838 - val_loss: 0.8373 - val_acc: 0.7220\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.6122 - acc: 0.7886 - val_loss: 0.8142 - val_acc: 0.7286\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.6040 - acc: 0.7918 - val_loss: 0.6373 - val_acc: 0.7859\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 265s 170ms/step - loss: 0.5906 - acc: 0.7964 - val_loss: 0.6449 - val_acc: 0.7795\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 263s 168ms/step - loss: 0.5872 - acc: 0.7976 - val_loss: 0.6401 - val_acc: 0.7849\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 263s 168ms/step - loss: 0.5778 - acc: 0.8000 - val_loss: 0.5803 - val_acc: 0.8056\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 262s 168ms/step - loss: 0.5696 - acc: 0.8036 - val_loss: 0.5635 - val_acc: 0.8095\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 267s 171ms/step - loss: 0.5690 - acc: 0.8045 - val_loss: 0.6487 - val_acc: 0.7829\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 276s 177ms/step - loss: 0.5587 - acc: 0.8043 - val_loss: 0.5426 - val_acc: 0.8193\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 282s 181ms/step - loss: 0.5446 - acc: 0.8109 - val_loss: 0.5411 - val_acc: 0.8192\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5418 - acc: 0.8137 - val_loss: 0.5119 - val_acc: 0.8274\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5349 - acc: 0.8170 - val_loss: 0.5260 - val_acc: 0.8199\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5268 - acc: 0.8188 - val_loss: 0.5680 - val_acc: 0.8129\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 243s 156ms/step - loss: 0.5207 - acc: 0.8216 - val_loss: 0.5417 - val_acc: 0.8169\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5164 - acc: 0.8226 - val_loss: 0.6691 - val_acc: 0.7795\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5126 - acc: 0.8236 - val_loss: 0.6316 - val_acc: 0.7882\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5085 - acc: 0.8241 - val_loss: 0.5150 - val_acc: 0.8236\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.5060 - acc: 0.8253 - val_loss: 0.5729 - val_acc: 0.8064\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 243s 156ms/step - loss: 0.4990 - acc: 0.8285 - val_loss: 0.5200 - val_acc: 0.8234\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4920 - acc: 0.8293 - val_loss: 0.6074 - val_acc: 0.7961\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4937 - acc: 0.8292 - val_loss: 0.5103 - val_acc: 0.8266\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4870 - acc: 0.8323 - val_loss: 0.5586 - val_acc: 0.8123\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4784 - acc: 0.8344 - val_loss: 0.5114 - val_acc: 0.8282\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4723 - acc: 0.8373 - val_loss: 0.4930 - val_acc: 0.8361\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.4694 - acc: 0.8382 - val_loss: 0.5036 - val_acc: 0.8290\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 260s 166ms/step - loss: 0.4674 - acc: 0.8380 - val_loss: 0.5830 - val_acc: 0.8070\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 0.4639 - acc: 0.8406 - val_loss: 0.5684 - val_acc: 0.8145\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 256s 164ms/step - loss: 0.4621 - acc: 0.8415 - val_loss: 0.5240 - val_acc: 0.8258\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.4558 - acc: 0.8430 - val_loss: 0.4895 - val_acc: 0.8364\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 0.4492 - acc: 0.8448 - val_loss: 0.5648 - val_acc: 0.8146\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 260s 167ms/step - loss: 0.4473 - acc: 0.8451 - val_loss: 0.5801 - val_acc: 0.8090\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 254s 163ms/step - loss: 0.4378 - acc: 0.8501 - val_loss: 0.5387 - val_acc: 0.8240\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 260s 166ms/step - loss: 0.4386 - acc: 0.8484 - val_loss: 0.5681 - val_acc: 0.8148\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 263s 168ms/step - loss: 0.4353 - acc: 0.8490 - val_loss: 0.4974 - val_acc: 0.8310\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 256s 164ms/step - loss: 0.4342 - acc: 0.8491 - val_loss: 0.5321 - val_acc: 0.8238\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 282s 181ms/step - loss: 0.4341 - acc: 0.8483 - val_loss: 0.5379 - val_acc: 0.8250\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 257s 165ms/step - loss: 0.4225 - acc: 0.8547 - val_loss: 0.5031 - val_acc: 0.8331\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.4269 - acc: 0.8528 - val_loss: 0.5325 - val_acc: 0.8272\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 254s 162ms/step - loss: 0.4229 - acc: 0.8538 - val_loss: 0.4973 - val_acc: 0.8319\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.4211 - acc: 0.8526 - val_loss: 0.5088 - val_acc: 0.8288\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 253s 162ms/step - loss: 0.4144 - acc: 0.8582 - val_loss: 0.5146 - val_acc: 0.8314\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4096 - acc: 0.8587 - val_loss: 0.4796 - val_acc: 0.8420\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4076 - acc: 0.8585 - val_loss: 0.5037 - val_acc: 0.8293\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4049 - acc: 0.8610 - val_loss: 0.5290 - val_acc: 0.8266\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4101 - acc: 0.8584 - val_loss: 0.5551 - val_acc: 0.8170\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4070 - acc: 0.8598 - val_loss: 0.4820 - val_acc: 0.8383\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.4020 - acc: 0.8619 - val_loss: 0.5008 - val_acc: 0.8359\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.3991 - acc: 0.8636 - val_loss: 0.5420 - val_acc: 0.8243\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 244s 156ms/step - loss: 0.3918 - acc: 0.8646 - val_loss: 0.5303 - val_acc: 0.8279\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 251s 161ms/step - loss: 0.3897 - acc: 0.8656 - val_loss: 0.5112 - val_acc: 0.8343\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 260s 167ms/step - loss: 0.3915 - acc: 0.8654 - val_loss: 0.5266 - val_acc: 0.8282\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 263s 168ms/step - loss: 0.3861 - acc: 0.8672 - val_loss: 0.5633 - val_acc: 0.8202\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 264s 169ms/step - loss: 0.3845 - acc: 0.8652 - val_loss: 0.4636 - val_acc: 0.8493\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.3855 - acc: 0.8685 - val_loss: 0.4873 - val_acc: 0.8414\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 249s 159ms/step - loss: 0.3802 - acc: 0.8691 - val_loss: 0.5338 - val_acc: 0.8240\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 266s 170ms/step - loss: 0.3770 - acc: 0.8700 - val_loss: 0.6028 - val_acc: 0.8069\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 0.3730 - acc: 0.8731 - val_loss: 0.4817 - val_acc: 0.8398\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 254s 162ms/step - loss: 0.3772 - acc: 0.8690 - val_loss: 0.4994 - val_acc: 0.8377\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 264s 169ms/step - loss: 0.3775 - acc: 0.8705 - val_loss: 0.5137 - val_acc: 0.8334\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 254s 163ms/step - loss: 0.3695 - acc: 0.8736 - val_loss: 0.4865 - val_acc: 0.8368\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 260s 167ms/step - loss: 0.3714 - acc: 0.8708 - val_loss: 0.5301 - val_acc: 0.8288\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 264s 169ms/step - loss: 0.3702 - acc: 0.8723 - val_loss: 0.5219 - val_acc: 0.8289\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 270s 173ms/step - loss: 0.3625 - acc: 0.8744 - val_loss: 0.4762 - val_acc: 0.8459\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 266s 170ms/step - loss: 0.3637 - acc: 0.8741 - val_loss: 0.5273 - val_acc: 0.8285\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.3635 - acc: 0.8749 - val_loss: 0.4988 - val_acc: 0.8406\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 256s 164ms/step - loss: 0.3600 - acc: 0.8760 - val_loss: 0.5214 - val_acc: 0.8273\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 253s 162ms/step - loss: 0.3598 - acc: 0.8742 - val_loss: 0.4860 - val_acc: 0.8416\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 255s 163ms/step - loss: 0.3567 - acc: 0.8772 - val_loss: 0.4681 - val_acc: 0.8432\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 258s 165ms/step - loss: 0.3516 - acc: 0.8796 - val_loss: 0.4895 - val_acc: 0.8423\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 254s 163ms/step - loss: 0.3555 - acc: 0.8770 - val_loss: 0.4593 - val_acc: 0.8470\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.3486 - acc: 0.8789 - val_loss: 0.5018 - val_acc: 0.8387\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 260s 166ms/step - loss: 0.3469 - acc: 0.8812 - val_loss: 0.5040 - val_acc: 0.8384\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 264s 169ms/step - loss: 0.3512 - acc: 0.8784 - val_loss: 0.5222 - val_acc: 0.8312\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 260s 167ms/step - loss: 0.3464 - acc: 0.8802 - val_loss: 0.5341 - val_acc: 0.8292\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 263s 168ms/step - loss: 0.3499 - acc: 0.8781 - val_loss: 0.5607 - val_acc: 0.8253\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 270s 173ms/step - loss: 0.3440 - acc: 0.8814 - val_loss: 0.4823 - val_acc: 0.8416\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 271s 173ms/step - loss: 0.3397 - acc: 0.8832 - val_loss: 0.5140 - val_acc: 0.8325\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 263s 168ms/step - loss: 0.3413 - acc: 0.8834 - val_loss: 0.4873 - val_acc: 0.8418\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 268s 172ms/step - loss: 0.3409 - acc: 0.8825 - val_loss: 0.5647 - val_acc: 0.8184\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 257s 164ms/step - loss: 0.3396 - acc: 0.8817 - val_loss: 0.5530 - val_acc: 0.8263\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 261s 167ms/step - loss: 0.3404 - acc: 0.8810 - val_loss: 0.4910 - val_acc: 0.8416\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 253s 162ms/step - loss: 0.3394 - acc: 0.8832 - val_loss: 0.5068 - val_acc: 0.8372\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 270s 173ms/step - loss: 0.3386 - acc: 0.8838 - val_loss: 0.5130 - val_acc: 0.8345\n",
      "Saved trained model at /home/aditya/Downloads/Keras/saved_models/keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 15s 1ms/step\n",
      "Test loss: 0.5129709065675735\n",
      "Test accuracy: 0.8345\n"
     ]
    }
   ],
   "source": [
    "datagen.fit(x_train)\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
